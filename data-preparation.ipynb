{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Diffusion Benchmark\n",
    "\n",
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aioboto3 in ./.venv/lib/python3.10/site-packages (11.3.1)\n",
      "Requirement already satisfied: boto3 in ./.venv/lib/python3.10/site-packages (1.28.17)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (2.1.1)\n",
      "Requirement already satisfied: plotly in ./.venv/lib/python3.10/site-packages (5.17.0)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (2.31.0)\n",
      "Requirement already satisfied: pillow in ./.venv/lib/python3.10/site-packages (10.1.0)\n",
      "Requirement already satisfied: tabulate in ./.venv/lib/python3.10/site-packages (0.9.0)\n",
      "Requirement already satisfied: kaleido in ./.venv/lib/python3.10/site-packages (0.2.1)\n",
      "Requirement already satisfied: nbformat in ./.venv/lib/python3.10/site-packages (5.9.2)\n",
      "Requirement already satisfied: ipython in ./.venv/lib/python3.10/site-packages (8.16.1)\n",
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.10/site-packages (1.0.0)\n",
      "Requirement already satisfied: aiobotocore==2.6.0 in ./.venv/lib/python3.10/site-packages (from aiobotocore[boto3]==2.6.0->aioboto3) (2.6.0)\n",
      "Requirement already satisfied: botocore<1.31.18,>=1.31.17 in ./.venv/lib/python3.10/site-packages (from aiobotocore==2.6.0->aiobotocore[boto3]==2.6.0->aioboto3) (1.31.17)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.7.4.post0 in ./.venv/lib/python3.10/site-packages (from aiobotocore==2.6.0->aiobotocore[boto3]==2.6.0->aioboto3) (3.8.6)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in ./.venv/lib/python3.10/site-packages (from aiobotocore==2.6.0->aiobotocore[boto3]==2.6.0->aioboto3) (1.15.0)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in ./.venv/lib/python3.10/site-packages (from aiobotocore==2.6.0->aiobotocore[boto3]==2.6.0->aioboto3) (0.11.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in ./.venv/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in ./.venv/lib/python3.10/site-packages (from boto3) (0.6.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./.venv/lib/python3.10/site-packages (from pandas) (1.26.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.venv/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in ./.venv/lib/python3.10/site-packages (from plotly) (8.2.3)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from plotly) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: fastjsonschema in ./.venv/lib/python3.10/site-packages (from nbformat) (2.18.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in ./.venv/lib/python3.10/site-packages (from nbformat) (4.19.1)\n",
      "Requirement already satisfied: jupyter-core in ./.venv/lib/python3.10/site-packages (from nbformat) (5.4.0)\n",
      "Requirement already satisfied: traitlets>=5.1 in ./.venv/lib/python3.10/site-packages (from nbformat) (5.12.0)\n",
      "Requirement already satisfied: backcall in ./.venv/lib/python3.10/site-packages (from ipython) (0.2.0)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.10/site-packages (from ipython) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.10/site-packages (from ipython) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.10/site-packages (from ipython) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in ./.venv/lib/python3.10/site-packages (from ipython) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in ./.venv/lib/python3.10/site-packages (from ipython) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.10/site-packages (from ipython) (2.16.1)\n",
      "Requirement already satisfied: stack-data in ./.venv/lib/python3.10/site-packages (from ipython) (0.6.3)\n",
      "Requirement already satisfied: exceptiongroup in ./.venv/lib/python3.10/site-packages (from ipython) (1.1.3)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.10/site-packages (from ipython) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in ./.venv/lib/python3.10/site-packages (from jedi>=0.16->ipython) (0.8.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./.venv/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.venv/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./.venv/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./.venv/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat) (0.10.6)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.10/site-packages (from pexpect>4.3->ipython) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython) (0.2.8)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./.venv/lib/python3.10/site-packages (from jupyter-core->nbformat) (3.11.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.10/site-packages (from stack-data->ipython) (2.0.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.10/site-packages (from stack-data->ipython) (2.4.0)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.10/site-packages (from stack-data->ipython) (0.2.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.7.4.post0->aiobotocore==2.6.0->aiobotocore[boto3]==2.6.0->aioboto3) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.7.4.post0->aiobotocore==2.6.0->aiobotocore[boto3]==2.6.0->aioboto3) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.7.4.post0->aiobotocore==2.6.0->aiobotocore[boto3]==2.6.0->aioboto3) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.7.4.post0->aiobotocore==2.6.0->aiobotocore[boto3]==2.6.0->aioboto3) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.7.4.post0->aiobotocore==2.6.0->aiobotocore[boto3]==2.6.0->aioboto3) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install aioboto3 boto3 pandas plotly requests pillow tabulate kaleido nbformat ipython python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the env and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import queue_jobs, salad_org_id, salad_project_name, reporting_api_key, reporting_url, queue_service_url, salad_headers, salad_api_base_url, delete_all_container_groups, start_all_container_groups, stop_all_container_groups, deep_merge, get_signed_upload_url, purge_all_queues, delete_all_container_groups_with_status, format_gpu_name, delete_empty_queues, list_all_container_groups\n",
    "import requests\n",
    "import os\n",
    "import dotenv\n",
    "import re\n",
    "import copy\n",
    "import uuid\n",
    "import itertools\n",
    "dotenv.load_dotenv(\".env\", override=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up container groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 GTX 1650 (4 GB)\n",
      "1 RTX 4080 (16 GB)\n",
      "2 RTX 2070 (8 GB)\n",
      "3 RTX 3060 Ti (8 GB)\n",
      "4 RTX 2080 (8 GB)\n",
      "5 GTX 1660 (6 GB)\n",
      "6 RTX 4070 Ti (12 GB)\n",
      "7 RTX 3060 (12 GB)\n",
      "8 GTX 1050 Ti (4 GB)\n",
      "9 RTX 2080 Ti (11 GB)\n",
      "10 GTX 1660 Super (6 GB)\n",
      "11 RTX 3080 Ti (12 GB)\n",
      "12 GTX 1060 (6 GB)\n",
      "13 RTX 3050 (8 GB)\n",
      "14 RTX 4070 (12 GB)\n",
      "15 RTX 3080 (10 GB)\n",
      "16 RTX 2060 (6 GB)\n",
      "17 GTX 1070 (8 GB)\n",
      "18 RTX 4090 (24 GB)\n",
      "19 RTX 3090 Ti (24 GB)\n",
      "20 RTX 3090 (24 GB)\n",
      "21 RTX 3070 Ti (8 GB)\n",
      "22 RTX 3070 (8 GB)\n"
     ]
    }
   ],
   "source": [
    "vcpu = 2\n",
    "memory = 1024 * 12\n",
    "\n",
    "replica_count_per_group = 5\n",
    "\n",
    "create_container_group_payload = {\n",
    "  \"name\": \"replace-this\",\n",
    "  \"replicas\": replica_count_per_group,\n",
    "  \"autostart_policy\": False,\n",
    "  \"container\": {\n",
    "    \"image\": \"replaceme:latest\",\n",
    "    \"resources\": {\n",
    "      \"cpu\": vcpu,\n",
    "      \"memory\": memory,\n",
    "      \"gpu_classes\": []\n",
    "    },\n",
    "    \"environment_variables\": {\n",
    "        \"REPORTING_API_KEY\": reporting_api_key,\n",
    "        \"REPORTING_URL\": reporting_url,\n",
    "        \"QUEUE_URL\": queue_service_url,\n",
    "        \"QUEUE_API_KEY\": reporting_api_key,\n",
    "        \"STARTUP_CHECK_MAX_TRIES\": \"1000\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "def get_gpu_classes():\n",
    "    url = f\"{salad_api_base_url}/organizations/{salad_org_id}/gpu-classes\"\n",
    "    response = requests.get(url, headers=salad_headers)\n",
    "    return [gpu for gpu in response.json()[\"items\"] if gpu[\"name\"] != \"Stable Diffusion Compatible\"]\n",
    "\n",
    "\n",
    "def create_container_group(name, image, gpu, env={}, dry_run=False):\n",
    "    payload = copy.deepcopy(create_container_group_payload)\n",
    "    payload[\"name\"] = name\n",
    "    payload[\"container\"][\"image\"] = image\n",
    "    payload[\"container\"][\"resources\"][\"gpu_classes\"] = [gpu]\n",
    "    payload[\"container\"][\"environment_variables\"].update(env)\n",
    "    url = f\"{salad_api_base_url}/organizations/{salad_org_id}/projects/{salad_project_name}/containers\"\n",
    "\n",
    "    if dry_run:\n",
    "        print(url)\n",
    "        print(payload)\n",
    "        return\n",
    "    response = requests.post(url, headers=salad_headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "gpu_classes = get_gpu_classes()\n",
    "\n",
    "\n",
    "images = {\n",
    "  \"stable-fast\": {\n",
    "    \"baked\": \"saladtechnologies/stable-fast-qr-code:worker0.1.0-0.5.0-baked\",\n",
    "    \"dynamic\": \"saladtechnologies/stable-fast-qr-code:worker0.1.0-0.5.0\"\n",
    "  },\n",
    "  \"sdnext\": {\n",
    "    \"baked\": \"saladtechnologies/sdnext:worker0.1.0-122143-128713\",\n",
    "    \"dynamic\": \"saladtechnologies/sdnext:worker0.1.0-dynamic\"\n",
    "  },\n",
    "  \"a1111\": {\n",
    "    \"baked\": \"saladtechnologies/a1111:worker0.1.0-122143-128713\",\n",
    "    \"dynamic\": \"saladtechnologies/a1111:worker0.1.0-dynamic\"\n",
    "  },\n",
    "  \"comfy\": {\n",
    "    \"baked\": \"saladtechnologies/comfyui:worker0.1.0-baked\",\n",
    "    \"dynamic\": \"saladtechnologies/comfyui:worker0.1.0-dynamic\"\n",
    "  },\n",
    "}\n",
    "\n",
    "env = {\n",
    "  \"stable-fast\": {\n",
    "    \"baked\": {},\n",
    "    \"dynamic\": {\n",
    "      \"CIVITAI_CONTROLNET_MODEL\": \"122143\",\n",
    "      \"CIVITAI_CHECKPOINT_MODEL\": \"128713\"\n",
    "    }\n",
    "  },\n",
    "  \"sdnext\": {\n",
    "    \"baked\": {},\n",
    "    \"dynamic\": {\n",
    "      \"CIVITAI_MODEL_VERSION_IDS\": \"122143,128713\"\n",
    "    }\n",
    "  },\n",
    "  \"a1111\": {\n",
    "    \"baked\": {},\n",
    "    \"dynamic\": {\n",
    "      \"CIVITAI_MODEL_VERSION_IDS\": \"122143,128713\"\n",
    "    }\n",
    "  },\n",
    "  \"comfy\": {\n",
    "    \"baked\": {},\n",
    "    \"dynamic\": {\n",
    "      \"CIVITAI_MODEL_VERSION_IDS\": \"122143,128713\"\n",
    "    }\n",
    "  },\n",
    "}\n",
    "\n",
    "queue_names = []\n",
    "container_groups = []\n",
    "\n",
    "# delete_all_container_groups()\n",
    "\n",
    "for i, gpu in enumerate(gpu_classes):\n",
    "    print(i, gpu[\"name\"])\n",
    "\n",
    "attempt = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create All of the Container Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating container group stable-fast-rtx3090ti24gb-baked-768...\n",
      "Creating container group sdnext-rtx3090ti24gb-baked-768...\n",
      "Creating container group a1111-rtx3090ti24gb-baked-768...\n",
      "Creating container group comfy-rtx3090ti24gb-baked-768...\n",
      "Found 20 container groups.\n",
      "4 queues to be filled\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def all_combos():\n",
    "  for gpu in gpu_classes:\n",
    "    for image_name, image in images.items():\n",
    "      for baked in [\"baked\", \"dynamic\"]:\n",
    "        for image_size in [\"512\", \"768\"]:\n",
    "          yield gpu, image_name, image, baked, image_size\n",
    "\n",
    "def one_gpu_class(gpu_index=0, resolutions=[\"512\"], baked_variants=[\"baked\", \"dynamic\"]):\n",
    "  for image_name, image in images.items():\n",
    "    for baked in baked_variants:\n",
    "      for image_size in resolutions:\n",
    "        yield gpu_classes[gpu_index], image_name, image, baked, image_size\n",
    "\n",
    "def create_all_container_groups(combos):\n",
    "  global attempt\n",
    "  for gpu, image_name, image, baked, image_size in combos:\n",
    "      env_vars = env[image_name][baked]\n",
    "      name = f\"{image_name}-{format_gpu_name(gpu['name'])}-{baked}-{image_size}\"\n",
    "      env_vars[\"QUEUE_NAME\"] = name\n",
    "      env_vars[\"BENCHMARK_ID\"] = name\n",
    "      env_vars[\"IMAGE_SIZE\"] = image_size\n",
    "      print(f\"Creating container group {name}...\")\n",
    "      queue_names.append(name)\n",
    "      create_container_group(f\"{name}-{attempt}\", image[baked], gpu[\"id\"], env=env_vars)\n",
    "      container_groups.append(f\"{name}-{attempt}\")\n",
    "\n",
    "# stop_all_container_groups()\n",
    "# delete_all_container_groups()\n",
    "# delete_all_container_groups_with_status(\"stopped\")\n",
    "# attempt += 1\n",
    "\n",
    "\n",
    "############ 1xxx cards ##############\n",
    "# GTX 1050 Ti (6 GB)\n",
    "# create_all_container_groups(one_gpu_class(8, resolutions=[\"512\",\"768\"]))\n",
    "\n",
    "# GTX 1060 (6 GB)\n",
    "# create_all_container_groups(one_gpu_class(12, resolutions=[\"512\",\"768\"]))\n",
    "\n",
    "# GTX 1070 (8 GB)\n",
    "# create_all_container_groups(one_gpu_class(17, resolutions=[\"512\",\"768\"]))\n",
    "\n",
    "# GTX 1650 (4GB)\n",
    "# create_all_container_groups(one_gpu_class(0, resolutions=[\"512\"]))\n",
    "# create_all_container_groups(one_gpu_class(0, resolutions=[\"768\"]))\n",
    "\n",
    "# GTX 1660 (6 GB)\n",
    "# create_all_container_groups(one_gpu_class(5, resolutions=[\"512\"]))\n",
    "# create_all_container_groups(one_gpu_class(5, resolutions=[\"768\"]))\n",
    "\n",
    "# GTX 1660 Super (6 GB)\n",
    "# create_all_container_groups(one_gpu_class(10, resolutions=[\"512\",\"768\"]))\n",
    "\n",
    "\n",
    "############ 2xxx cards ##############\n",
    "# RTX 2060 (6 GB)\n",
    "# create_all_container_groups(one_gpu_class(16, resolutions=[\"512\",\"768\"]))\n",
    "\n",
    "# GTX 2070 (8 GB)\n",
    "# create_all_container_groups(one_gpu_class(2, resolutions=[\"512\",\"768\"]))\n",
    "\n",
    "# RTX 2080 (8 GB)\n",
    "# create_all_container_groups(one_gpu_class(4, resolutions=[\"512\",\"768\"])) # running\n",
    "\n",
    "# RTX 2080 Ti (11 GB)\n",
    "# create_all_container_groups(one_gpu_class(9, resolutions=[\"512\",\"768\"]))\n",
    "\n",
    "\n",
    "############ 3xxx cards ##############\n",
    "# RTX 3050 (8 GB)\n",
    "# create_all_container_groups(one_gpu_class(13, resolutions=[\"512\"])) #\n",
    "# create_all_container_groups(one_gpu_class(13, resolutions=[\"768\"])) # running\n",
    "\n",
    "# RTX 3060 (12 GB)\n",
    "# create_all_container_groups(one_gpu_class(7, resolutions=[\"512\"])) #\n",
    "# create_all_container_groups(one_gpu_class(7, resolutions=[\"768\"])) #\n",
    "\n",
    "# RTX 3060 Ti (8 GB)\n",
    "# create_all_container_groups(one_gpu_class(3, resolutions=[\"512\"])) #\n",
    "# create_all_container_groups(one_gpu_class(3, resolutions=[\"768\"])) #\n",
    "\n",
    "# RTX 3070 (8 GB)\n",
    "# create_all_container_groups(one_gpu_class(22, resolutions=[\"512\"])) #\n",
    "# create_all_container_groups(one_gpu_class(22, resolutions=[\"768\"], baked_variants=[\"baked\"]))\n",
    "# create_all_container_groups(one_gpu_class(22, resolutions=[\"768\"], baked_variants=[\"dynamic\"])) \n",
    "\n",
    "# RTX 3070 Ti (8 GB)\n",
    "# create_all_container_groups(one_gpu_class(21, resolutions=[\"512\"])) #\n",
    "# create_all_container_groups(one_gpu_class(21, resolutions=[\"768\"], baked_variants=[\"baked\"]))\n",
    "# create_all_container_groups(one_gpu_class(21, resolutions=[\"768\"], baked_variants=[\"dynamic\"]))\n",
    "\n",
    "# RTX 3080 (10 GB)\n",
    "# create_all_container_groups(one_gpu_class(15, resolutions=[\"512\"])) #\n",
    "# create_all_container_groups(one_gpu_class(15, resolutions=[\"768\"], baked_variants=[\"baked\"]))\n",
    "# create_all_container_groups(one_gpu_class(15, resolutions=[\"768\"], baked_variants=[\"dynamic\"]))\n",
    "\n",
    "# RTX 3080 Ti (12 GB)\n",
    "# create_all_container_groups(one_gpu_class(11, resolutions=[\"512\"])) #\n",
    "# create_all_container_groups(one_gpu_class(11, resolutions=[\"768\"])) #\n",
    "\n",
    "# RTX 3090 (24 GB)\n",
    "# create_all_container_groups(one_gpu_class(20, resolutions=[\"512\"])) #\n",
    "# create_all_container_groups(one_gpu_class(20, resolutions=[\"768\"], baked_variants=[\"baked\"]))\n",
    "# create_all_container_groups(one_gpu_class(20, resolutions=[\"768\"], baked_variants=[\"dynamic\"])) \n",
    "\n",
    "# RTX 3090 Ti (24 GB)\n",
    "# create_all_container_groups(one_gpu_class(19, resolutions=[\"512\"])) #\n",
    "# create_all_container_groups(one_gpu_class(19, resolutions=[\"768\"], baked_variants=[\"baked\"])) # running\n",
    "# create_all_container_groups(one_gpu_class(19, resolutions=[\"768\"], baked_variants=[\"dynamic\"])) \n",
    "\n",
    "\n",
    "############ 4xxx cards ##############\n",
    "# RTX 4070 (12 GB)\n",
    "# create_all_container_groups(one_gpu_class(14, resolutions=[\"512\"])) #\n",
    "# create_all_container_groups(one_gpu_class(14, resolutions=[\"768\"])) #\n",
    "\n",
    "# RTX 4070 Ti (12 GB)\n",
    "# create_all_container_groups(one_gpu_class(6, resolutions=[\"512\"])) #\n",
    "# create_all_container_groups(one_gpu_class(6, resolutions=[\"768\"], baked_variants=[\"baked\"]))\n",
    "# create_all_container_groups(one_gpu_class(6, resolutions=[\"768\"], baked_variants=[\"dynamic\"]))\n",
    "\n",
    "# RTX 4080 (16 GB)\n",
    "# create_all_container_groups(one_gpu_class(1, resolutions=[\"512\"])) #\n",
    "# create_all_container_groups(one_gpu_class(1, resolutions=[\"768\"], baked_variants=[\"baked\"]))\n",
    "# create_all_container_groups(one_gpu_class(1, resolutions=[\"768\"], baked_variants=[\"dynamic\"]))\n",
    "\n",
    "# RTX 4090 (24 GB)\n",
    "# create_all_container_groups(one_gpu_class(18, resolutions=[\"512\"], baked_variants=[\"baked\"])) #\n",
    "# create_all_container_groups(one_gpu_class(18, resolutions=[\"512\"], baked_variants=[\"dynamic\"])) #\n",
    "# create_all_container_groups(one_gpu_class(18, resolutions=[\"768\"], baked_variants=[\"baked\"]))\n",
    "# create_all_container_groups(one_gpu_class(18, resolutions=[\"768\"], baked_variants=[\"dynamic\"]))\n",
    "\n",
    "\n",
    "start_all_container_groups()\n",
    "\n",
    "print(f\"{len(queue_names)} queues to be filled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 600 jobs for 1400 images.\n"
     ]
    }
   ],
   "source": [
    "base_qr_payload = {\n",
    "    \"batch_size\": 1,\n",
    "    \"upload_url\": [],\n",
    "    \"stable_diffusion_params\": {\n",
    "        \"controlnet_conditioning_scale\": 2.0,\n",
    "        \"guidance_scale\": 4.0,\n",
    "        \"control_guidance_start\": 0.1,\n",
    "        \"control_guidance_end\": 0.95,\n",
    "        \"negative_prompt\": \"\"\n",
    "    },\n",
    "    \"qr_params\": {\n",
    "        \"drawer\": \"RoundedModule\",\n",
    "        \"error_correction\": \"H\",\n",
    "        \"color_mask\": \"SolidFill\",\n",
    "        \"color_mask_params\": {\"front_color\": [0, 0, 0], \"back_color\": [127, 127, 127]},\n",
    "    },\n",
    "}\n",
    "\n",
    "variants = [\n",
    "    {\n",
    "        \"stable_diffusion_params\": {\n",
    "            \"prompt\": \"leafy green salad\",\n",
    "        },\n",
    "        \"qr_params\": {\n",
    "            \"data\": \"https://salad.com\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"stable_diffusion_params\": {\n",
    "            \"prompt\": \"clouds\",\n",
    "        },\n",
    "        \"qr_params\": {\n",
    "            \"data\": \"https://salad.com\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"stable_diffusion_params\": {\n",
    "            \"prompt\": \"ocean\",\n",
    "        },\n",
    "        \"qr_params\": {\n",
    "            \"data\": \"https://salad.com\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"stable_diffusion_params\": {\n",
    "            \"prompt\": \"Pizza Pepperoni\",\n",
    "            \"control_guidance_end\": 1.0,\n",
    "            \"control_guidance_start\": 0.05,\n",
    "        },\n",
    "        \"qr_params\": {\n",
    "            \"data\": \"https://salad.com/\",\n",
    "            \"color_mask_params\": {\n",
    "              \"back_color\": [184, 184, 184]\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"stable_diffusion_params\": {\"prompt\": \"fruit salad\", \"guidance_scale\": 3.8},\n",
    "        \"qr_params\": {\n",
    "            \"data\": \"https://salad.com/pricing\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"stable_diffusion_params\": {\n",
    "            \"prompt\": \"leafy green salad\",\n",
    "            \"guidance_scale\": 3.7,\n",
    "        },\n",
    "        \"qr_params\": {\n",
    "            \"data\": \"https://salad.com/pricing\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"stable_diffusion_params\": {\n",
    "            \"prompt\": \"Pizza Pepperoni\",\n",
    "            \"control_guidance_end\": 1.0,\n",
    "            \"control_guidance_start\": 0.05,\n",
    "        },\n",
    "        \"qr_params\": {\n",
    "            \"data\": \"https://salad.com/pricing\",\n",
    "            \"color_mask_params\": {\n",
    "              \"back_color\": [184, 184, 184]\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"stable_diffusion_params\": {\n",
    "            \"prompt\": \"fire\",\n",
    "        },\n",
    "        \"qr_params\": {\n",
    "            \"data\": \"https://salad.com/download\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"stable_diffusion_params\": {\n",
    "            \"prompt\": \"galaxy\",\n",
    "        },\n",
    "        \"qr_params\": {\n",
    "            \"data\": \"https://salad.com/download\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"stable_diffusion_params\": {\n",
    "            \"prompt\": \"gold coins\",\n",
    "        },\n",
    "        \"qr_params\": {\n",
    "            \"data\": \"https://salad.com/download\",\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "# This will get run for every container group\n",
    "def get_all_jobs(num_per_variant=10):\n",
    "    for variant in variants:\n",
    "        base = copy.deepcopy(base_qr_payload)\n",
    "        payload = deep_merge(base, variant)\n",
    "        for num_steps in [15, 50]:\n",
    "            for batch_size in [1,2,4]:\n",
    "                for i in range(num_per_variant):\n",
    "                    job = copy.deepcopy(payload)\n",
    "                    job[\"id\"] = str(uuid.uuid4())\n",
    "                    job[\"stable_diffusion_params\"][\"num_inference_steps\"] = num_steps\n",
    "                    job[\"batch_size\"] = batch_size\n",
    "                    yield job\n",
    "\n",
    "jobs = list(get_all_jobs())\n",
    "num_images = sum([job[\"batch_size\"] for job in jobs])\n",
    "print(f\"Created {len(jobs)} jobs for {num_images} images.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queueing stable-fast jobs for stable-fast-rtx3090ti24gb-baked-768...\n",
      "{'status': 'No Messages Found', 'messages': []}\n",
      "Queueing jobs for queue benchmark-stable-fast-rtx3090ti24gb-baked-768.fifo...\n",
      "Sent 600 jobs in total.\n",
      "Queueing sdnext jobs for sdnext-rtx3090ti24gb-baked-768...\n",
      "{'status': 'No Messages Found', 'messages': []}\n",
      "Queueing jobs for queue benchmark-sdnext-rtx3090ti24gb-baked-768.fifo...\n",
      "Sent 600 jobs in total.\n",
      "Queueing a1111 jobs for a1111-rtx3090ti24gb-baked-768...\n",
      "{'status': 'No Messages Found', 'messages': []}\n",
      "Queueing jobs for queue benchmark-a1111-rtx3090ti24gb-baked-768.fifo...\n",
      "Sent 600 jobs in total.\n",
      "Queueing comfy jobs for comfy-rtx3090ti24gb-baked-768...\n",
      "{'status': 'No Messages Found', 'messages': []}\n",
      "Queueing jobs for queue benchmark-comfy-rtx3090ti24gb-baked-768.fifo...\n",
      "Sent 600 jobs in total.\n"
     ]
    }
   ],
   "source": [
    "def get_all_jobs_for_backend(backend: str = \"stable-fast\"):\n",
    "  image_ext = \"jpg\"\n",
    "  image_type = \"image/jpeg\"\n",
    "  if backend == \"comfy\":\n",
    "    image_ext = \"png\"\n",
    "    image_type = \"image/png\"\n",
    "  for job in get_all_jobs():\n",
    "    for i in range(job[\"batch_size\"]):\n",
    "      job[\"upload_url\"].append(get_signed_upload_url(f\"{job['id']}-{i}.{image_ext}\", image_type))\n",
    "    yield job\n",
    "\n",
    "\n",
    "def get_backend_from_queue_name(queue_name: str):\n",
    "  \"\"\"\n",
    "  stable-fast-gtx16504gb-baked-512 -> stable-fast\n",
    "  a1111-rtx309024gb-dynamic-768 -> a1111\n",
    "  \"\"\"\n",
    "  queue_name_parts = queue_name.split(\"-\")\n",
    "  return \"-\".join(queue_name_parts[:-3])\n",
    "\n",
    "# await purge_all_queues(queue_names)\n",
    "\n",
    "for queue_name in queue_names:\n",
    "  # if \"512\" in queue_name:\n",
    "  #   continue\n",
    "  backend = get_backend_from_queue_name(queue_name)\n",
    "  print(f\"Queueing {backend} jobs for {queue_name}...\")\n",
    "  await queue_jobs(queue_name, get_all_jobs_for_backend(backend), delay=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting queue benchmark-a1111-rtx20808gb-baked-768.fifo...\n",
      "Deleting queue benchmark-comfy-rtx409024gb-dynamic-512.fifo...\n",
      "Deleting queue benchmark-sdnext-rtx409024gb-dynamic-512.fifo...\n",
      "Deleting queue benchmark-stable-fast-rtx409024gb-dynamic-512.fifo...\n",
      "Found 12 container groups.\n",
      "comfy-rtx20808gb-dynamic-768-1 - 1 running - 0 creating - 4 allocating\n",
      "sdnext-rtx20808gb-baked-512-1 - 1 running - 2 creating - 2 allocating\n",
      "sdnext-rtx20808gb-dynamic-512-1 - 0 running - 0 creating - 5 allocating\n",
      "sdnext-rtx20808gb-dynamic-768-1 - 0 running - 0 creating - 5 allocating\n",
      "stable-fast-rtx20808gb-baked-768-1 - 1 running - 1 creating - 3 allocating\n",
      "a1111-rtx30508gb-dynamic-768-0 - 3 running - 0 creating - 2 allocating\n",
      "comfy-rtx30508gb-baked-768-0 - 0 running - 0 creating - 5 allocating\n",
      "comfy-rtx30508gb-dynamic-768-0 - 0 running - 0 creating - 5 allocating\n",
      "sdnext-rtx30508gb-baked-768-0 - 1 running - 0 creating - 4 allocating\n",
      "sdnext-rtx30508gb-dynamic-768-0 - 3 running - 1 creating - 1 allocating\n",
      "stable-fast-rtx30508gb-baked-768-0 - 2 running - 1 creating - 2 allocating\n",
      "stable-fast-rtx30508gb-dynamic-768-0 - 1 running - 0 creating - 4 allocating\n",
      "Total running: 13\n",
      "Total allocating: 42\n",
      "Total creating: 5\n"
     ]
    }
   ],
   "source": [
    "# def get_benchmark_ids(combos):\n",
    "#   for gpu, image_name, image, baked, image_size in combos:\n",
    "#     yield f\"{image_name}-{format_gpu_name(gpu['name'])}-{baked}-{image_size}\"\n",
    "\n",
    "# # Write all benchmark ids to a file\n",
    "# with open(\"benchmark_ids.txt\", \"w\") as f:\n",
    "#   for benchmark_id in get_benchmark_ids(all_combos()):\n",
    "#     f.write(f\"{benchmark_id}\\n\")\n",
    "await delete_empty_queues()\n",
    "\n",
    "def sort_by_gpu_class(data):\n",
    "    # Define a function to extract the GPU class\n",
    "    def gpu_class_key(item):\n",
    "        name = item[\"name\"]\n",
    "        match = re.search(r'\\wtx\\d+(ti)?\\d+gb', name)\n",
    "        return match.group(0) if match else \"\"\n",
    "\n",
    "    # Sort the list using the GPU class as the key\n",
    "    sorted_data = sorted(data, key=gpu_class_key)\n",
    "    return sorted_data\n",
    "\n",
    "\n",
    "total_running = 0\n",
    "total_allocating = 0\n",
    "total_creating = 0\n",
    "for container_group in sort_by_gpu_class(list_all_container_groups(\"running\")):\n",
    "    name = container_group[\"name\"]\n",
    "    running_count = container_group[\"current_state\"][\"instance_status_count\"][\"running_count\"]\n",
    "    allocating_count = container_group[\"current_state\"][\"instance_status_count\"][\"allocating_count\"]\n",
    "    creating_count = container_group[\"current_state\"][\"instance_status_count\"][\"creating_count\"]\n",
    "    total_running += running_count\n",
    "    total_allocating += allocating_count\n",
    "    total_creating += creating_count\n",
    "    print(\n",
    "        f\"{name} - {running_count} running - {creating_count} creating - {allocating_count} allocating\"\n",
    "    )\n",
    "print(f\"Total running: {total_running}\")\n",
    "print(f\"Total allocating: {total_allocating}\")\n",
    "print(f\"Total creating: {total_creating}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
